#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯­æ–™åº“æ„å»ºå™¨
è¯»å–å¹´æŠ¥HTMLï¼Œç”¨LLMæå–äº”ç»´åº¦æ•°æ®å’Œæ–‡æœ¬æ®µè½
"""

import asyncio
import json
import re
import logging
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup
import httpx
from openai import AsyncOpenAI

from database_manager_v2 import DatabaseManagerV2
from config import OPENAI_CONFIG, CORPUS_EXTRACTION_PROMPT, QUANTITATIVE_CONFIG
from ticker_resolver import resolve_ticker

# é…ç½®æ—¥å¿—
log_dir = Path("/root/liujie/nianbao-v2/logs")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f"corpus_builder_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CorpusBuilder:
    """è¯­æ–™åº“æ„å»ºå™¨"""
    
    def __init__(self, db_path: str):
        self.db = DatabaseManagerV2(db_path)
        self.client = AsyncOpenAI(
            base_url=OPENAI_CONFIG["base_url"],
            api_key=OPENAI_CONFIG["api_key"],
            http_client=httpx.AsyncClient(
                base_url=OPENAI_CONFIG["base_url"],
                follow_redirects=True,
            ),
        )
        self.max_concurrent = QUANTITATIVE_CONFIG["max_concurrent"]
    
    def parse_html(self, html_file: Path) -> str:
        """è§£æHTMLæ–‡ä»¶ï¼Œæå–æ–‡æœ¬å†…å®¹"""
        with open(html_file, 'r', encoding='utf-8', errors='ignore') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
        
        # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
        for tag in soup(['script', 'style', 'meta', 'link']):
            tag.decompose()
        
        # æå–æ–‡æœ¬
        text = soup.get_text(separator='\n', strip=True)
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        return text
    
    def extract_metadata(self, html_file: Path) -> Dict[str, Any]:
        """ä»æ–‡ä»¶åæå–å…ƒæ•°æ®"""
        filename = html_file.stem
        parts = filename.split('_')
        
        if len(parts) >= 6:
            # æ–‡ä»¶åæ ¼å¼: cik_fiscal_year_end_ticker_company_name_parts..._form_type_report_date
            # å…¬å¸åå¯èƒ½åŒ…å«å¤šä¸ªä¸‹åˆ’çº¿åˆ†éš”çš„éƒ¨åˆ†ï¼ˆåŸæœ¬æ˜¯ç©ºæ ¼ï¼‰
            # parts[0]: cik
            # parts[1]: fiscal_year_end
            # parts[2]: ticker
            # parts[3:-2]: company_name (å¤šä¸ªéƒ¨åˆ†ç”¨ç©ºæ ¼è¿æ¥)
            # parts[-2]: form_type (å¦‚ 10-K)
            # parts[-1]: report_date
            
            company_name_parts = parts[3:-2]
            company_name = ' '.join(company_name_parts)
            
            return {
                'cik': parts[0],
                'fiscal_year_end': parts[1],
                'ticker': parts[2],
                'company_name': company_name,
                'report_date': parts[-1],
                'data_year': int(parts[1].split('-')[0]),
                'needs_ticker_resolution': parts[2].lower() in ['none', 'unknown', ''] or not parts[2]
            }
        
        return {
            'cik': '',
            'ticker': filename[:10],
            'company_name': 'Unknown',
            'fiscal_year_end': '',
            'report_date': '',
            'data_year': 2025,
            'needs_ticker_resolution': True
        }
    
    def estimate_tokens(self, text: str) -> int:
        """ç²—ç•¥ä¼°ç®—tokenæ•°ï¼ˆ1 token â‰ˆ 4 å­—ç¬¦ï¼‰"""
        return len(text) // 4
    
    def split_text_into_chunks(self, text: str, max_tokens: int = 100000) -> list:
        """
        å°†é•¿æ–‡æœ¬åˆ†å—ï¼Œä¿æŒæ®µè½å®Œæ•´æ€§
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            max_tokens: æ¯å—æœ€å¤§tokenæ•°ï¼ˆå®é™…æŒ‰max_chars=max_tokens*4è®¡ç®—ï¼‰
        """
        max_chars = max_tokens * 4  # tokenåˆ°å­—ç¬¦çš„è½¬æ¢
        
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        current_chunk = ""
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡max_charsï¼Œå¼ºåˆ¶åˆ†å‰²
            if len(para) > max_chars:
                # å…ˆä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # å°†è¶…é•¿æ®µè½æŒ‰å­—ç¬¦å¼ºåˆ¶åˆ†å‰²
                for i in range(0, len(para), max_chars):
                    chunks.append(para[i:i+max_chars])
                continue
            
            # æ­£å¸¸çš„æ®µè½åˆ†å—é€»è¾‘
            if len(current_chunk) + len(para) + 2 <= max_chars:
                current_chunk += para + '\n\n'
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + '\n\n'
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    async def llm_extract(self, text: str, max_retries: int = 3, ticker: str = "UNKNOWN") -> Dict:
        """ä½¿ç”¨LLMæå–ç»“æ„åŒ–æ•°æ®å’Œæ–‡æœ¬æ®µè½ï¼ˆå¸¦é‡è¯•ï¼‰"""
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        estimated_tokens = self.estimate_tokens(text)
        max_tokens = 100000  # GPT-4-turbo æ”¯æŒ 128Kï¼Œç•™ä¸€äº›ä½™é‡
        
        # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œåˆ†å—å¤„ç†
        if estimated_tokens > max_tokens:
            logger.warning(f"[{ticker}] æ–‡æœ¬è¿‡é•¿ (ä¼°è®¡ {estimated_tokens:,} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†...")
            print(f"  âš ï¸ æ–‡æœ¬è¿‡é•¿ (ä¼°è®¡ {estimated_tokens:,} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†...")
            return await self.llm_extract_chunked(text, ticker)
        
        # å¸¦é‡è¯•çš„æ­£å¸¸å¤„ç†
        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=OPENAI_CONFIG["model"],
                    messages=[{
                        "role": "user",
                        "content": CORPUS_EXTRACTION_PROMPT.format(text=text)
                    }],
                    temperature=OPENAI_CONFIG["temperature"],
                    response_format={"type": "json_object"}
                )
                
                content = response.choices[0].message.content
                result = json.loads(content)
                
                # éªŒè¯ç»“æœ
                if not result.get("text_segments"):
                    logger.warning(f"[{ticker}] æå–ç»“æœä¸ºç©º (å°è¯• {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print(f"  âš ï¸ æå–ç»“æœä¸ºç©ºï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                        await asyncio.sleep(2)
                        continue
                    else:
                        logger.error(f"[{ticker}] æ‰€æœ‰é‡è¯•åä»ç„¶è¿”å›ç©ºç»“æœ")
                
                logger.info(f"[{ticker}] æˆåŠŸæå– {len(result.get('text_segments', []))} ä¸ªæ®µè½")
                return result
                
            except json.JSONDecodeError as e:
                logger.error(f"[{ticker}] JSONè§£æé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    print(f"  âš ï¸ JSONè§£æé”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2)
                    continue
                else:
                    print(f"  âŒ JSONæœ€ç»ˆè§£æå¤±è´¥: {e}")
                    logger.error(f"[{ticker}] JSONè§£ææœ€ç»ˆå¤±è´¥: {e}")
                    return {"structured_data": {}, "text_segments": []}
                    
            except Exception as e:
                error_msg = str(e)
                logger.error(f"[{ticker}] LLMè°ƒç”¨é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}: {error_msg}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯tokené™åˆ¶é”™è¯¯
                if 'maximum context length' in error_msg.lower() or 'token' in error_msg.lower():
                    print(f"  âš ï¸ Tokené™åˆ¶é”™è¯¯ï¼Œåˆ‡æ¢åˆ°åˆ†å—å¤„ç†...")
                    return await self.llm_extract_chunked(text, ticker)
                
                if attempt < max_retries - 1:
                    print(f"  âš ï¸ LLMé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}")
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"  âŒ LLMæœ€ç»ˆå¤±è´¥: {type(e).__name__}: {str(e)}")
                    print(f"     æ¨¡å‹: {OPENAI_CONFIG['model']}")
                    logger.error(f"[{ticker}] LLMæœ€ç»ˆå¤±è´¥: {type(e).__name__}: {error_msg}, æ¨¡å‹: {OPENAI_CONFIG['model']}")
                    return {"structured_data": {}, "text_segments": []}
    
    async def llm_extract_chunked(self, text: str, ticker: str = "UNKNOWN") -> Dict:
        """åˆ†å—å¤„ç†é•¿æ–‡æœ¬"""
        chunks = self.split_text_into_chunks(text, max_tokens=100000)
        logger.info(f"[{ticker}] åˆ†æˆ {len(chunks)} å—å¤„ç†...")
        print(f"  ğŸ“¦ åˆ†æˆ {len(chunks)} å—å¤„ç†...")
        
        all_structured_data = {}
        all_text_segments = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"     å¤„ç†ç¬¬ {i}/{len(chunks)} å—...")
            logger.info(f"[{ticker}] å¤„ç†ç¬¬ {i}/{len(chunks)} å—...")
            # ä½¿ç”¨å¸¦é‡è¯•çš„æ–¹æ³•
            result = await self._llm_extract_single_chunk(chunk, ticker)
            
            # åˆå¹¶ç»“æ„åŒ–æ•°æ®
            structured = result.get('structured_data', {})
            for key, value in structured.items():
                if key not in all_structured_data:
                    all_structured_data[key] = value if isinstance(value, list) else [value]
                elif isinstance(value, list):
                    all_structured_data[key].extend(value)
                else:
                    if isinstance(all_structured_data[key], list):
                        all_structured_data[key].append(value)
                    else:
                        all_structured_data[key] = [all_structured_data[key], value]
            
            # åˆå¹¶æ–‡æœ¬æ®µè½
            segments = result.get('text_segments', [])
            all_text_segments.extend(segments)
        
        logger.info(f"[{ticker}] åˆ†å—å¤„ç†å®Œæˆï¼šåˆå¹¶äº† {len(chunks)} å—æ•°æ®ï¼Œå…± {len(all_text_segments)} ä¸ªæ®µè½")
        print(f"  âœ“ åˆ†å—å¤„ç†å®Œæˆï¼šåˆå¹¶äº† {len(chunks)} å—æ•°æ®")
        
        return {
            "structured_data": all_structured_data,
            "text_segments": all_text_segments
        }
    
    async def _llm_extract_single_chunk(self, text: str, ticker: str = "UNKNOWN", max_retries: int = 3) -> Dict:
        """å•ä¸ªå—çš„LLMæå–ï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=OPENAI_CONFIG["model"],
                    messages=[{
                        "role": "user",
                        "content": CORPUS_EXTRACTION_PROMPT.format(text=text)
                    }],
                    temperature=OPENAI_CONFIG["temperature"],
                    response_format={"type": "json_object"}
                )
                
                content = response.choices[0].message.content
                result = json.loads(content)
                
                # éªŒè¯ç»“æœ
                if not result.get("text_segments") and attempt < max_retries - 1:
                    logger.warning(f"[{ticker}] å—æå–ç»“æœä¸ºç©º (å°è¯• {attempt + 1}/{max_retries})")
                    print(f"        âš ï¸ ç©ºç»“æœï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2)
                    continue
                
                return result
                
            except json.JSONDecodeError as e:
                logger.error(f"[{ticker}] å—JSONè§£æé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    print(f"        âš ï¸ JSONé”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2)
                    continue
                return {"structured_data": {}, "text_segments": []}
                    
            except Exception as e:
                logger.error(f"[{ticker}] å—æå–é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}: {str(e)}")
                if attempt < max_retries - 1:
                    print(f"        âš ï¸ {type(e).__name__}ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2 ** attempt)
                    continue
                print(f"        âŒ æœ€ç»ˆå¤±è´¥: {type(e).__name__}")
                return {"structured_data": {}, "text_segments": []}
    
    async def build_corpus_for_report(self, html_file: Path):
        """ä¸ºå•ä¸ªå¹´æŠ¥æ„å»ºè¯­æ–™åº“"""
        logger.info(f"å¼€å§‹å¤„ç†: {html_file.name}")
        print(f"\nå¤„ç†: {html_file.name}")
        
        try:
            # 1. æå–å…ƒæ•°æ®
            metadata = self.extract_metadata(html_file)
            ticker = metadata['ticker']
            data_year = metadata['data_year']
            
            # 2. Tickeræ™ºèƒ½è¡¥å…¨
            if metadata.get('needs_ticker_resolution', False):
                logger.info(f"[{ticker}] æ£€æµ‹åˆ°tickerä¸ºNone/Unknownï¼Œå¯åŠ¨æ™ºèƒ½è¡¥å…¨...")
                print(f"  æ£€æµ‹åˆ°tickerä¸ºNone/Unknownï¼Œå¯åŠ¨æ™ºèƒ½è¡¥å…¨...")
                resolved_ticker = await resolve_ticker(metadata['company_name'], metadata['cik'])
                if resolved_ticker:
                    ticker = resolved_ticker
                    logger.info(f"[{ticker}] Tickerè¡¥å…¨æˆåŠŸ: {ticker}")
                    print(f"  âœ“ Tickerè¡¥å…¨æˆåŠŸ: {ticker}")
                else:
                    ticker = f"UNKNOWN_{metadata['cik']}"
                    logger.warning(f"[{ticker}] æ— æ³•è§£ætickerï¼Œä½¿ç”¨: {ticker}")
                    print(f"  âš ï¸ æ— æ³•è§£ætickerï¼Œä½¿ç”¨: {ticker}")
            
            logger.info(f"[{ticker}] å…¬å¸: {metadata['company_name']}, å¹´ä»½: {data_year}")
            print(f"  å…¬å¸: {metadata['company_name']} ({ticker})")
            print(f"  å¹´ä»½: {data_year}")
            
            # 2. è§£æHTML
            print(f"  è§£æHTML...")
            text = self.parse_html(html_file)
            estimated_tokens = self.estimate_tokens(text)
            logger.info(f"[{ticker}] æå–æ–‡æœ¬: {len(text):,} å­—ç¬¦ (ä¼°è®¡ ~{estimated_tokens:,} tokens)")
            print(f"  æå–æ–‡æœ¬: {len(text):,} å­—ç¬¦ (ä¼°è®¡ ~{estimated_tokens:,} tokens)")
            
            # 3. LLMåˆ†æ
            print(f"  LLMåˆ†æä¸­...")
            logger.info(f"[{ticker}] å¼€å§‹LLMåˆ†æ...")
            result = await self.llm_extract(text, ticker=ticker)
            
            structured_data = result.get('structured_data', {})
            text_segments = result.get('text_segments', [])
            
            logger.info(f"[{ticker}] ç»“æ„åŒ–æ•°æ®: {len(structured_data)} ä¸ªç»´åº¦, æ–‡æœ¬æ®µè½: {len(text_segments)} ä¸ª")
            print(f"  âœ“ ç»“æ„åŒ–æ•°æ®: {len(structured_data)} ä¸ªç»´åº¦")
            print(f"  âœ“ æ–‡æœ¬æ®µè½: {len(text_segments)} ä¸ª")
            
            # æ£€æŸ¥æ˜¯å¦æå–å¤±è´¥
            if len(text_segments) == 0:
                logger.error(f"[{ticker}] âŒ æå–å¤±è´¥ï¼šæ²¡æœ‰æ–‡æœ¬æ®µè½")
                print(f"  âŒ è­¦å‘Šï¼šæ²¡æœ‰æå–åˆ°æ–‡æœ¬æ®µè½")
            
            # 4. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæŒ‰å¹´ä»½ç‹¬ç«‹è¡¨ï¼‰
            self.db.save_company(
                ticker=ticker,
                data_year=data_year,
                cik=metadata['cik'],
                company_name=metadata['company_name'],
                fiscal_year_end=metadata['fiscal_year_end'],
                report_date=metadata['report_date'],
                file_path=str(html_file)
            )
            
            self.db.save_structured_data(ticker, data_year, structured_data)
            self.db.save_text_segments(ticker, data_year, text_segments)
            
            logger.info(f"[{ticker}] âœ… å¤„ç†å®Œæˆ")
            
            return {
                'ticker': ticker,
                'company_name': metadata['company_name'],
                'segments_count': len(text_segments)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {html_file.name}: {type(e).__name__}: {str(e)}", exc_info=True)
            print(f"  âŒ å¤„ç†å¤±è´¥: {type(e).__name__}: {str(e)}")
            raise
    
    async def build_corpus_batch(self, html_files: list):
        """æ‰¹é‡å¤„ç†å¤šä¸ªå¹´æŠ¥ï¼ˆå¹¶å‘ï¼‰"""
        logger.info(f"{'='*80}")
        logger.info(f"è¯­æ–™åº“æ„å»ºå™¨ - æ‰¹é‡å¤„ç†å¼€å§‹")
        logger.info(f"æ–‡ä»¶æ•°é‡: {len(html_files)}, å¹¶å‘æ•°: {self.max_concurrent}")
        logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        logger.info(f"{'='*80}")
        
        print(f"\n{'='*80}")
        print(f"è¯­æ–™åº“æ„å»ºå™¨ - æ‰¹é‡å¤„ç†")
        print(f"{'='*80}")
        print(f"æ–‡ä»¶æ•°é‡: {len(html_files)}")
        print(f"å¹¶å‘æ•°: {self.max_concurrent}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_with_semaphore(file):
            async with semaphore:
                return await self.build_corpus_for_report(file)
        
        # å¹¶å‘å¤„ç†
        tasks = [process_with_semaphore(f) for f in html_files]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if isinstance(r, dict) and r.get('segments_count', 0) > 0)
        failed_count = sum(1 for r in results if isinstance(r, dict) and r.get('segments_count', 0) == 0)
        error_count = sum(1 for r in results if not isinstance(r, dict))
        
        logger.info(f"{'='*80}")
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ={success_count}, æ•°æ®ä¸ºç©º={failed_count}, å¼‚å¸¸={error_count}")
        logger.info(f"{'='*80}")
        
        print(f"\n{'='*80}")
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ")
        print(f"  âœ… æˆåŠŸï¼ˆæœ‰æ•°æ®ï¼‰: {success_count}/{len(html_files)}")
        print(f"  âš ï¸  æ•°æ®ä¸ºç©º: {failed_count}")
        print(f"  âŒ å¼‚å¸¸é”™è¯¯: {error_count}")
        print(f"  ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶
        if failed_count > 0 or error_count > 0:
            print(f"\nå¤±è´¥çš„æ–‡ä»¶:")
            for i, (result, file) in enumerate(zip(results, html_files)):
                if isinstance(result, dict):
                    if result.get('segments_count', 0) == 0:
                        print(f"  âš ï¸  {result.get('ticker', 'UNKNOWN')}: {file.name} (æ•°æ®ä¸ºç©º)")
                elif isinstance(result, Exception):
                    print(f"  âŒ {file.name}: {type(result).__name__}: {str(result)}")
        
        return results


def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python corpus_builder.py <data_dir> [limit]")
        return
    
    data_dir = Path(sys.argv[1])
    limit = int(sys.argv[2]) if len(sys.argv) > 2 else None
    
    db_path = "/root/liujie/nianbao-v2results/annual_reports_quantitative.db"
    builder = CorpusBuilder(db_path)
    
    # è·å–HTMLæ–‡ä»¶
    html_files = sorted(data_dir.glob("*.html"))
    if limit:
        html_files = html_files[:limit]
    
    # è¿è¡Œæ‰¹é‡å¤„ç†
    asyncio.run(builder.build_corpus_batch(html_files))


if __name__ == "__main__":
    main()

