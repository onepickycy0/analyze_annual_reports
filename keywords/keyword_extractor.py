#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…³é”®è¯æå–å™¨ - æŒ‰å¹´ä»½æå–ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰
"""

import asyncio
import json
import logging
from typing import List, Dict
from pathlib import Path
from datetime import datetime
import httpx
from openai import AsyncOpenAI

from database_manager_v2 import DatabaseManagerV2
from config import OPENAI_CONFIG, KEYWORD_EXTRACTION_PROMPT, QUANTITATIVE_CONFIG

# é…ç½®æ—¥å¿—
log_dir = Path("/root/liujie/nianbao-v2/logs")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f"keyword_extractor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class KeywordExtractor:
    """å…³é”®è¯æå–å™¨ - æŒ‰å¹´ä»½ç‹¬ç«‹æå–ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, db_path: str):
        self.db = DatabaseManagerV2(db_path)
        self.client = AsyncOpenAI(
            base_url=OPENAI_CONFIG["base_url"],
            api_key=OPENAI_CONFIG["api_key"],
            http_client=httpx.AsyncClient(
                base_url=OPENAI_CONFIG["base_url"],
                follow_redirects=True,
            ),
        )
        self.batch_size = QUANTITATIVE_CONFIG["batch_size_keywords"]
        self.max_concurrent = QUANTITATIVE_CONFIG.get("max_concurrent", 5)
        logger.info(f"åˆå§‹åŒ–å…³é”®è¯æå–å™¨: batch_size={self.batch_size}, max_concurrent={self.max_concurrent}")
    
    async def llm_extract_keywords(self, texts: List[str], batch_id: int = 0, max_retries: int = 3) -> List[Dict]:
        """ä½¿ç”¨LLMæå–å…³é”®è¯ï¼ˆå¸¦é‡è¯•ï¼‰"""
        corpus_text = "\n\n---\n\n".join(texts[:50])
        
        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=OPENAI_CONFIG["model"],
                    messages=[{"role": "user", "content": KEYWORD_EXTRACTION_PROMPT.format(corpus_texts=corpus_text)}],
                    temperature=OPENAI_CONFIG["temperature"],
                    response_format={"type": "json_object"}
                )
                keywords = json.loads(response.choices[0].message.content).get('keywords', [])
                
                if not keywords and attempt < max_retries - 1:
                    logger.warning(f"æ‰¹æ¬¡{batch_id}: ç©ºç»“æœï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    print(f"      âš ï¸ æ‰¹æ¬¡{batch_id}: ç©ºç»“æœï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2)
                    continue
                
                logger.info(f"æ‰¹æ¬¡{batch_id}: æˆåŠŸæå– {len(keywords)} ä¸ªå…³é”®è¯")
                return keywords
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡{batch_id}: é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}: {str(e)}")
                if attempt < max_retries - 1:
                    print(f"      âš ï¸ æ‰¹æ¬¡{batch_id}: é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {type(e).__name__}")
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    print(f"      âŒ æ‰¹æ¬¡{batch_id}: æœ€ç»ˆå¤±è´¥: {e}")
                    return []
    
    async def extract_keywords_for_year(self, data_year: int) -> Dict[str, Dict]:
        """ä¸ºæŒ‡å®šå¹´ä»½æå–å…³é”®è¯ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
        logger.info(f"å¼€å§‹æå– {data_year} å¹´å…³é”®è¯")
        print(f"\n  ã€{data_year}å¹´ã€‘æå–å…³é”®è¯...")
        
        corpus_data = self.db.get_corpus_texts_by_year(data_year)
        texts = [item['text'] for item in corpus_data if item['text']]
        
        if not texts:
            logger.warning(f"{data_year}å¹´æ— è¯­æ–™åº“æ•°æ®")
            print(f"    âš ï¸ æ— è¯­æ–™åº“æ•°æ®")
            return {}
        
        logger.info(f"{data_year}å¹´è¯­æ–™åº“: {len(texts)} æ®µæ–‡æœ¬")
        print(f"    è¯­æ–™åº“: {len(texts)} æ®µ")
        
        # åˆ†æ‰¹æå–
        batches = [texts[i:i+self.batch_size] for i in range(0, len(texts), self.batch_size)]
        logger.info(f"{data_year}å¹´åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œå¹¶å‘æ•°: {self.max_concurrent}")
        print(f"    åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆå¹¶å‘æ•°: {self.max_concurrent}ï¼‰")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def extract_with_semaphore(batch_id: int, batch: List[str]):
            async with semaphore:
                return await self.llm_extract_keywords(batch, batch_id)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        tasks = [extract_with_semaphore(i+1, batch) for i, batch in enumerate(batches)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        all_keywords = {}
        success_count = 0
        
        for i, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹æ¬¡{i}å¼‚å¸¸: {type(result).__name__}: {str(result)}")
                print(f"      âŒ æ‰¹æ¬¡{i}å¼‚å¸¸")
                continue
            
            if not result:
                continue
            
            success_count += 1
            for kw in result:
                kw_text = kw.get('keyword', '').strip().lower()
                if kw_text and kw_text not in all_keywords:
                    all_keywords[kw_text] = {
                        'keyword': kw_text,
                        'category': kw.get('category', 'unknown'),
                        'context': kw.get('context', ''),
                        'method': 'llm'
                    }
        
        # ç»Ÿè®¡
        categories = {}
        for kw in all_keywords.values():
            cat = kw['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        logger.info(f"{data_year}å¹´: æˆåŠŸæ‰¹æ¬¡ {success_count}/{len(batches)}, æå– {len(all_keywords)} ä¸ªå…³é”®è¯")
        print(f"    âœ“ æˆåŠŸæ‰¹æ¬¡: {success_count}/{len(batches)}")
        print(f"    âœ“ æå– {len(all_keywords)} ä¸ªå…³é”®è¯")
        for cat, count in sorted(categories.items()):
            print(f"      - {cat}: {count}")
        
        return all_keywords
    
    async def extract_keywords_from_corpus(self) -> Dict[int, Dict[str, Dict]]:
        """æŒ‰å¹´ä»½æå–æ‰€æœ‰å…³é”®è¯ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
        logger.info("="*80)
        logger.info("å…³é”®è¯æå–å™¨å¯åŠ¨ - æŒ‰å¹´ä»½ç‹¬ç«‹æå–ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰")
        logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
        logger.info("="*80)
        
        print(f"\n{'='*80}")
        print(f"å…³é”®è¯æå–å™¨ - æŒ‰å¹´ä»½ç‹¬ç«‹æå–ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰")
        print(f"{'='*80}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        
        years = self.db.get_available_years()
        logger.info(f"å¯ç”¨å¹´ä»½: {years}")
        print(f"\nå¯ç”¨å¹´ä»½: {years}")
        
        if not years:
            logger.warning("æ— å¯ç”¨æ•°æ®")
            print("âš ï¸ æ— æ•°æ®")
            return {}
        
        all_years_keywords = {}
        
        for year in years:
            year_keywords = await self.extract_keywords_for_year(year)
            all_years_keywords[year] = year_keywords
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if year_keywords:
                logger.info(f"{year}å¹´: ä¿å­˜ {len(year_keywords)} ä¸ªå…³é”®è¯åˆ°æ•°æ®åº“")
                self.db.save_keywords(list(year_keywords.values()))
            else:
                logger.warning(f"{year}å¹´: æ— å…³é”®è¯å¯ä¿å­˜")
        
        total_keywords = sum(len(kws) for kws in all_years_keywords.values())
        logger.info("="*80)
        logger.info(f"å®Œæˆ - å…± {len(years)} å¹´ï¼Œ{total_keywords} ä¸ªå…³é”®è¯")
        logger.info("="*80)
        
        print(f"\n{'='*80}")
        print(f"âœ… å®Œæˆ - å…±{len(years)}å¹´ï¼Œ{total_keywords}ä¸ªå…³é”®è¯")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
        print(f"{'='*80}")
        
        return all_years_keywords


def main():
    db_path = "/root/liujie/nianbao-v2results/annual_reports_quantitative.db"
    extractor = KeywordExtractor(db_path)
    asyncio.run(extractor.extract_keywords_from_corpus())


if __name__ == "__main__":
    main()
